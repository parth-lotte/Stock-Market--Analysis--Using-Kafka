
# Stock Market Real Time Ananlysis Using Apache Kafka

## Apache Kafka

**Apache Kafka** is a distributed data store optimized for ingesting and processing streaming data in real-time. Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously. A streaming platform needs to handle this constant influx of data, and process the data sequentially and incrementally. 

**Use Case**

Kafka is used to build **real-time** streaming data pipelines and **real-time streaming applications**. A  data pipeline reliably processes and moves data from one system to another, and a streaming application is an application that consumes streams of data.

#### Architecture

![alt text](https://github.com/parth-lotte/Stock-Market--Analysis--Using-Kafka/blob/main/architecture_kafka.png)


## Tech Stack used

* Python
* Pandas
* Apache Kafka 
* Amazon Web Services (AWS) 
* Jupyter Notebook
* AWS Components
  - EC2
  - Athena
  - Glue Crawler
  - Glue catlog
  - S3 Bucket

## Code Flow 

![alt text](https://github.com/parth-lotte/Stock-Market--Analysis--Using-Kafka/blob/main/Flowchart.png)


#### Dataset

The dataset file indexProcessed.csv contains 10 attributes **Index, Date, Open, High, Low, Close, Adj Close, Volume & CloseUSD.**

#### Project Execution

Firstly, We start the **Zookeeper and the Kafka server**. After that create a topic in the Kafka Server then execute the producer command in the Kafka Server with the consumer command open in another window.

This will allow us the real time processing of the data after being fed to the producer. In our project the data was fed to the **KafkaProducer.ipynb** with the following code :

    while True:
        dict_stock=df.sample(1).to_dict(orient="records")[0]
        producer.send('demo-testing',value=dict_stock)
        sleep(1)

Where as in the consumer file **(KafkaConsumer.ipynb)** the following code consume the data in real time :

    for count, i in enumerate(consumer):
    with s3.open("s3://stock-market-lotte/stock_market_{}.json".format(count),'w') as file: json.dump(i.value, file)    


We installed the Kafka Server on the  **Amazon EC2** for the computation purpose.

After the above steps we stored our data on **Amazon S3** Bucket , in the form of **stock_market.json** objects.

Later, with the help of crawler we scrape the data and using Amazon Glue we provide both visual and code-based interfaces to make data integration easier. 

In the end using **Amazon Athena** we execute our SQL queries on the data stored in Amazon S3 Bucket. 


## Amazon S3 Bucket

![alt text](https://github.com/parth-lotte/Stock-Market--Analysis--Using-Kafka/blob/main/S3Bucket.png)








## Authors

- [@parth-lotte](https://github.com/parth-lotte)

